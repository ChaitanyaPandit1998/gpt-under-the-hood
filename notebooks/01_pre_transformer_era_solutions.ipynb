{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Solutions: Pre-Transformer Era Exercises\n",
        "\n",
        "This notebook provides complete solutions to the exercises from **`01_pre_transformer_era.ipynb`**.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "Through these exercises, you'll deepen your understanding of:\n",
        "\n",
        "1. **RNN Architecture** - How hyperparameters and activation functions affect behavior\n",
        "2. **Vanishing Gradients** - Why this is an exponential problem and the narrow \"Goldilocks zone\"\n",
        "3. **LSTM Gates** - How gates dynamically control information flow\n",
        "4. **Conceptual Thinking** - Brainstorming solutions that lead to transformers\n",
        "5. **Language Modeling** - Building a complete character-level model with backpropagation\n",
        "\n",
        "## How to Use This Notebook\n",
        "\n",
        "- **Self-contained**: All necessary code is included (SimpleRNN, LSTMCell, etc.)\n",
        "- **Progressive**: Exercises build from simple to complex\n",
        "- **Visual**: Extensive visualizations help you understand concepts\n",
        "- **Educational**: Not just answers, but detailed explanations of \"why\"\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before starting, make sure you've completed:\n",
        "- \u2705 `00_introduction_and_setup.ipynb`\n",
        "- \u2705 `01_pre_transformer_era.ipynb`\n",
        "\n",
        "Let's dive in! \ud83d\ude80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Setup\n",
        "\n",
        "First, let's import all dependencies and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Add utilities to path\n",
        "sys.path.append(os.path.abspath('../src'))\n",
        "from utils import set_style, print_tensor_info\n",
        "\n",
        "# Set visual style for consistency\n",
        "set_style()\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\u2713 Setup complete! Libraries imported and random seed set.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions and Classes\n",
        "\n",
        "We'll copy the necessary classes from the main notebook so this notebook is self-contained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleRNN:\n",
        "    \"\"\"\n",
        "    A minimal RNN implementation in NumPy.\n",
        "    \n",
        "    This implements the equations:\n",
        "      h_t = activation(W_hh @ h_{t-1} + W_xh @ x_t + b_h)\n",
        "      y_t = W_hy @ h_t + b_y\n",
        "      \n",
        "    Modified to support different activation functions (tanh, ReLU).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, activation='tanh'):\n",
        "        \"\"\"\n",
        "        Initialize the RNN with random weights.\n",
        "        \n",
        "        Args:\n",
        "            input_size: Dimension of input vectors\n",
        "            hidden_size: Dimension of hidden state\n",
        "            output_size: Dimension of output\n",
        "            activation: 'tanh' or 'relu'\n",
        "        \"\"\"\n",
        "        self.hidden_size = hidden_size\n",
        "        self.activation = activation\n",
        "        \n",
        "        # Weight matrices\n",
        "        self.W_xh = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "        self.W_hy = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        \n",
        "        # Biases\n",
        "        self.b_h = np.zeros((hidden_size,))\n",
        "        self.b_y = np.zeros((output_size,))\n",
        "    \n",
        "    def _activate(self, x):\n",
        "        \"\"\"Apply activation function.\"\"\"\n",
        "        if self.activation == 'tanh':\n",
        "            return np.tanh(x)\n",
        "        elif self.activation == 'relu':\n",
        "            return np.maximum(0, x)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation: {self.activation}\")\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Forward pass through the RNN.\n",
        "        \n",
        "        Args:\n",
        "            inputs: Sequence of inputs, shape (seq_len, input_size)\n",
        "        \n",
        "        Returns:\n",
        "            outputs: Sequence of outputs, shape (seq_len, output_size)\n",
        "            hidden_states: All hidden states, shape (seq_len, hidden_size)\n",
        "        \"\"\"\n",
        "        seq_len = len(inputs)\n",
        "        hidden_states = []\n",
        "        outputs = []\n",
        "        \n",
        "        # Initial hidden state (all zeros)\n",
        "        h = np.zeros(self.hidden_size)\n",
        "        \n",
        "        # Process sequence\n",
        "        for t in range(seq_len):\n",
        "            x_t = inputs[t]\n",
        "            \n",
        "            # Update hidden state\n",
        "            hidden_contribution = self.W_hh.T @ h\n",
        "            input_contribution = self.W_xh.T @ x_t\n",
        "            combined = hidden_contribution + input_contribution + self.b_h\n",
        "            h = self._activate(combined)\n",
        "            \n",
        "            # Compute output\n",
        "            y = self.W_hy.T @ h + self.b_y\n",
        "            \n",
        "            hidden_states.append(h.copy())\n",
        "            outputs.append(y)\n",
        "        \n",
        "        return np.array(outputs), np.array(hidden_states)\n",
        "\n",
        "print(\"\u2713 SimpleRNN class loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip to prevent overflow\n",
        "\n",
        "class LSTMCell:\n",
        "    \"\"\"\n",
        "    Single LSTM cell (processes one timestep).\n",
        "    \n",
        "    Implements the full LSTM equations with forget, input, and output gates.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        \"\"\"Initialize LSTM cell with random weights.\"\"\"\n",
        "        self.hidden_size = hidden_size\n",
        "        combined_size = hidden_size + input_size\n",
        "        \n",
        "        # Weight matrices for gates and cell candidate\n",
        "        self.W_f = np.random.randn(combined_size, hidden_size) * 0.01\n",
        "        self.W_i = np.random.randn(combined_size, hidden_size) * 0.01\n",
        "        self.W_C = np.random.randn(combined_size, hidden_size) * 0.01\n",
        "        self.W_o = np.random.randn(combined_size, hidden_size) * 0.01\n",
        "        \n",
        "        # Biases\n",
        "        self.b_f = np.zeros(hidden_size)\n",
        "        self.b_i = np.zeros(hidden_size)\n",
        "        self.b_C = np.zeros(hidden_size)\n",
        "        self.b_o = np.zeros(hidden_size)\n",
        "    \n",
        "    def forward(self, x_t, h_prev, C_prev):\n",
        "        \"\"\"\n",
        "        Forward pass for one timestep.\n",
        "        \n",
        "        Args:\n",
        "            x_t: Input at time t\n",
        "            h_prev: Previous hidden state\n",
        "            C_prev: Previous cell state\n",
        "        \n",
        "        Returns:\n",
        "            h_t: New hidden state\n",
        "            C_t: New cell state\n",
        "            (f_t, i_t, o_t): Gate values\n",
        "        \"\"\"\n",
        "        # Concatenate previous hidden state and current input\n",
        "        combined = np.concatenate([h_prev, x_t])\n",
        "        \n",
        "        # Forget gate\n",
        "        f_t = sigmoid(combined @ self.W_f + self.b_f)\n",
        "        \n",
        "        # Input gate\n",
        "        i_t = sigmoid(combined @ self.W_i + self.b_i)\n",
        "        \n",
        "        # Cell candidate\n",
        "        C_tilde = np.tanh(combined @ self.W_C + self.b_C)\n",
        "        \n",
        "        # Update cell state (the key equation!)\n",
        "        C_t = f_t * C_prev + i_t * C_tilde\n",
        "        \n",
        "        # Output gate\n",
        "        o_t = sigmoid(combined @ self.W_o + self.b_o)\n",
        "        \n",
        "        # Compute hidden state\n",
        "        h_t = o_t * np.tanh(C_t)\n",
        "        \n",
        "        return h_t, C_t, (f_t, i_t, o_t)\n",
        "\n",
        "print(\"\u2713 LSTMCell class loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_gradient_flow(seq_len=50, weight_value=0.9):\n",
        "    \"\"\"\n",
        "    Simulate how gradients change as they flow backward through time in an RNN.\n",
        "    \n",
        "    Args:\n",
        "        seq_len: Length of sequence\n",
        "        weight_value: Value of recurrent weight W_hh\n",
        "    \n",
        "    Returns:\n",
        "        Array of gradient values at each timestep\n",
        "    \"\"\"\n",
        "    gradient = 1.0\n",
        "    gradients = [gradient]\n",
        "    tanh_derivative = 0.5  # Approximate average\n",
        "    \n",
        "    for t in range(seq_len - 1):\n",
        "        gradient = gradient * weight_value * tanh_derivative\n",
        "        gradients.append(gradient)\n",
        "    \n",
        "    return np.array(gradients[::-1])\n",
        "\n",
        "print(\"\u2713 Helper functions loaded\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"All setup complete! Ready to solve exercises.\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Exercise 1: Experiment with RNN\n",
        "\n",
        "## Original Exercise\n",
        "\n",
        "> **Task**: Modify the `SimpleRNN` class and observe the effects.\n",
        ">\n",
        "> Try:\n",
        "> - Change `hidden_size` to 2, 8, 16. How do the hidden state visualizations change?\n",
        "> - Replace `tanh` with `ReLU` (`np.maximum(0, x)`). What happens? Why might this be bad?\n",
        "> - Process a longer sequence (seq_len=20). How do hidden states evolve?\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Understand how `hidden_size` affects the RNN's representational capacity\n",
        "- See why activation functions matter (tanh vs ReLU)\n",
        "- Observe how RNNs process longer sequences\n",
        "\n",
        "Let's explore each part!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part A: Hidden Size Variations\n",
        "\n",
        "The `hidden_size` parameter controls how many numbers the RNN uses to remember information. Let's see how different sizes affect the behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create RNNs with different hidden sizes\n",
        "input_size = 3\n",
        "output_size = 2\n",
        "seq_len = 5\n",
        "hidden_sizes = [2, 8, 16]\n",
        "\n",
        "# Generate same input for fair comparison\n",
        "np.random.seed(42)\n",
        "inputs = np.random.randn(seq_len, input_size)\n",
        "\n",
        "# Store results for each hidden size\n",
        "results = {}\n",
        "\n",
        "for hidden_size in hidden_sizes:\n",
        "    rnn = SimpleRNN(input_size, hidden_size, output_size)\n",
        "    outputs, hidden_states = rnn.forward(inputs)\n",
        "    results[hidden_size] = hidden_states\n",
        "    print(f\"Hidden size {hidden_size:2d}: hidden_states shape = {hidden_states.shape}\")\n",
        "\n",
        "print(\"\\n\u2713 Processed sequence with 3 different hidden sizes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize hidden states for all three sizes\n",
        "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
        "\n",
        "for idx, hidden_size in enumerate(hidden_sizes):\n",
        "    hidden_states = results[hidden_size]\n",
        "    \n",
        "    # Left column: Line plot\n",
        "    ax_line = axes[idx, 0]\n",
        "    ax_line.plot(hidden_states, linewidth=2, marker='o', markersize=4)\n",
        "    ax_line.set_xlabel('Timestep', fontsize=11)\n",
        "    ax_line.set_ylabel('Hidden State Value', fontsize=11)\n",
        "    ax_line.set_title(f'Hidden Size = {hidden_size} (Line Plot)', fontsize=12, fontweight='bold')\n",
        "    ax_line.legend([f'h[{i}]' for i in range(hidden_size)], ncol=2, fontsize=8)\n",
        "    ax_line.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Right column: Heatmap\n",
        "    ax_heat = axes[idx, 1]\n",
        "    sns.heatmap(hidden_states.T, \n",
        "                cmap='coolwarm', \n",
        "                center=0,\n",
        "                xticklabels=range(seq_len),\n",
        "                yticklabels=[f'h[{i}]' for i in range(hidden_size)],\n",
        "                cbar_kws={'label': 'Value'},\n",
        "                annot=True if hidden_size <= 8 else False,  # Only annotate for smaller sizes\n",
        "                fmt='.2f',\n",
        "                ax=ax_heat)\n",
        "    ax_heat.set_xlabel('Timestep', fontsize=11)\n",
        "    ax_heat.set_ylabel('Hidden Dimension', fontsize=11)\n",
        "    ax_heat.set_title(f'Hidden Size = {hidden_size} (Heatmap)', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis: What Do We Observe?\n",
        "\n",
        "**Key Observations**:\n",
        "\n",
        "1. **Hidden Size = 2** (Small Capacity)\n",
        "   - Only 2 dimensions to store information\n",
        "   - Limited representational capacity\n",
        "   - Good for simple patterns, but can't capture complex relationships\n",
        "\n",
        "2. **Hidden Size = 8** (Medium Capacity)\n",
        "   - 8 dimensions provide more flexibility\n",
        "   - Can capture more nuanced patterns\n",
        "   - Good balance between capacity and efficiency\n",
        "\n",
        "3. **Hidden Size = 16** (Large Capacity)\n",
        "   - 16 dimensions offer high representational power\n",
        "   - Can model complex relationships\n",
        "   - But: more parameters = more data needed for training\n",
        "\n",
        "**Why This Matters**:\n",
        "- **Too small**: RNN can't capture enough information \u2192 underfitting\n",
        "- **Too large**: More parameters to train, slower, risk of overfitting\n",
        "- **Real-world**: Modern RNNs often use 256-1024 hidden dimensions for complex tasks like language modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part B: ReLU vs tanh Activation\n",
        "\n",
        "Activation functions are crucial in RNNs. Let's compare tanh (the standard choice) with ReLU (popular in feedforward networks)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create two RNNs: one with tanh, one with ReLU\n",
        "np.random.seed(42)\n",
        "input_size = 3\n",
        "hidden_size = 4\n",
        "output_size = 2\n",
        "seq_len = 10\n",
        "\n",
        "# Generate input\n",
        "inputs = np.random.randn(seq_len, input_size)\n",
        "\n",
        "# Create RNNs\n",
        "rnn_tanh = SimpleRNN(input_size, hidden_size, output_size, activation='tanh')\n",
        "rnn_relu = SimpleRNN(input_size, hidden_size, output_size, activation='relu')\n",
        "\n",
        "# Make them use the same weights for fair comparison\n",
        "rnn_relu.W_xh = rnn_tanh.W_xh.copy()\n",
        "rnn_relu.W_hh = rnn_tanh.W_hh.copy()\n",
        "rnn_relu.W_hy = rnn_tanh.W_hy.copy()\n",
        "rnn_relu.b_h = rnn_tanh.b_h.copy()\n",
        "rnn_relu.b_y = rnn_tanh.b_y.copy()\n",
        "\n",
        "# Forward pass\n",
        "outputs_tanh, hidden_tanh = rnn_tanh.forward(inputs)\n",
        "outputs_relu, hidden_relu = rnn_relu.forward(inputs)\n",
        "\n",
        "print(\"\u2713 Processed sequence with both tanh and ReLU\")\n",
        "print(f\"\\ntanh hidden states range: [{hidden_tanh.min():.3f}, {hidden_tanh.max():.3f}]\")\n",
        "print(f\"ReLU hidden states range: [{hidden_relu.min():.3f}, {hidden_relu.max():.3f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# tanh - Line plot\n",
        "axes[0, 0].plot(hidden_tanh, linewidth=2, marker='o', markersize=4)\n",
        "axes[0, 0].set_xlabel('Timestep')\n",
        "axes[0, 0].set_ylabel('Hidden State Value')\n",
        "axes[0, 0].set_title('tanh Activation (Line Plot)', fontweight='bold')\n",
        "axes[0, 0].legend([f'h[{i}]' for i in range(hidden_size)], ncol=2)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "axes[0, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "\n",
        "# tanh - Heatmap\n",
        "sns.heatmap(hidden_tanh.T, cmap='coolwarm', center=0, \n",
        "            xticklabels=range(seq_len), yticklabels=[f'h[{i}]' for i in range(hidden_size)],\n",
        "            cbar_kws={'label': 'Value'}, annot=True, fmt='.2f', ax=axes[0, 1])\n",
        "axes[0, 1].set_xlabel('Timestep')\n",
        "axes[0, 1].set_ylabel('Hidden Dimension')\n",
        "axes[0, 1].set_title('tanh Activation (Heatmap)', fontweight='bold')\n",
        "\n",
        "# ReLU - Line plot\n",
        "axes[1, 0].plot(hidden_relu, linewidth=2, marker='s', markersize=4)\n",
        "axes[1, 0].set_xlabel('Timestep')\n",
        "axes[1, 0].set_ylabel('Hidden State Value')\n",
        "axes[1, 0].set_title('ReLU Activation (Line Plot)', fontweight='bold')\n",
        "axes[1, 0].legend([f'h[{i}]' for i in range(hidden_size)], ncol=2)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "axes[1, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "\n",
        "# ReLU - Heatmap\n",
        "sns.heatmap(hidden_relu.T, cmap='coolwarm', center=0,\n",
        "            xticklabels=range(seq_len), yticklabels=[f'h[{i}]' for i in range(hidden_size)],\n",
        "            cbar_kws={'label': 'Value'}, annot=True, fmt='.2f', ax=axes[1, 1])\n",
        "axes[1, 1].set_xlabel('Timestep')\n",
        "axes[1, 1].set_ylabel('Hidden Dimension')\n",
        "axes[1, 1].set_title('ReLU Activation (Heatmap)', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis: Why ReLU is Problematic for RNNs\n",
        "\n",
        "**Observations**:\n",
        "\n",
        "1. **tanh** (Top row):\n",
        "   - Values bounded in [-1, 1]\n",
        "   - Smooth evolution over time\n",
        "   - Stable behavior\n",
        "\n",
        "2. **ReLU** (Bottom row):\n",
        "   - Values can be 0 or positive (no negative values)\n",
        "   - Some dimensions may \"die\" (stuck at 0)\n",
        "   - Can lead to unbounded growth (exploding activations)\n",
        "\n",
        "**Why ReLU is Bad for RNNs**:\n",
        "\n",
        "1. **Dying ReLU Problem**:\n",
        "   - If `W_hh @ h + W_xh @ x + b_h < 0`, then ReLU outputs 0\n",
        "   - That dimension becomes \"dead\" and can never recover\n",
        "   - You lose that dimension's capacity permanently\n",
        "\n",
        "2. **Exploding Activations**:\n",
        "   - ReLU has no upper bound (unlike tanh which saturates at 1)\n",
        "   - Repeated multiplication: `h_t = ReLU(W @ h_{t-1} + ...)` can cause values to grow exponentially\n",
        "   - This leads to numerical instability and NaN values\n",
        "\n",
        "3. **No Negative Values**:\n",
        "   - ReLU only outputs non-negative values\n",
        "   - This limits expressiveness - can't represent \"opposite\" relationships\n",
        "   - tanh's symmetric range [-1, 1] is more natural for modeling changes\n",
        "\n",
        "**Why This Matters**:\n",
        "- **tanh is the standard** for vanilla RNNs and LSTMs for good reasons!\n",
        "- Some variants use other activations (e.g., GRU uses sigmoid in gates)\n",
        "- Transformers avoid this issue entirely by not having recurrence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part C: Longer Sequences\n",
        "\n",
        "Let's see how RNNs behave when processing longer sequences (20 timesteps instead of 5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process a longer sequence\n",
        "np.random.seed(42)\n",
        "seq_len_long = 20\n",
        "inputs_long = np.random.randn(seq_len_long, input_size)\n",
        "\n",
        "# Use medium hidden size\n",
        "rnn_long = SimpleRNN(input_size, hidden_size=8, output_size=output_size)\n",
        "outputs_long, hidden_long = rnn_long.forward(inputs_long)\n",
        "\n",
        "print(f\"Processed sequence of length {seq_len_long}\")\n",
        "print(f\"Hidden states shape: {hidden_long.shape}\")\n",
        "print(f\"Hidden states range: [{hidden_long.min():.3f}, {hidden_long.max():.3f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize longer sequence\n",
        "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
        "\n",
        "# Line plot: All dimensions\n",
        "axes[0].plot(hidden_long, linewidth=2, marker='o', markersize=3)\n",
        "axes[0].set_xlabel('Timestep', fontsize=12)\n",
        "axes[0].set_ylabel('Hidden State Value', fontsize=12)\n",
        "axes[0].set_title('Hidden State Evolution Over 20 Timesteps', fontsize=13, fontweight='bold')\n",
        "axes[0].legend([f'h[{i}]' for i in range(8)], ncol=4, fontsize=9)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
        "\n",
        "# Heatmap\n",
        "sns.heatmap(hidden_long.T, cmap='coolwarm', center=0,\n",
        "            xticklabels=range(seq_len_long), yticklabels=[f'h[{i}]' for i in range(8)],\n",
        "            cbar_kws={'label': 'Value'}, ax=axes[1])\n",
        "axes[1].set_xlabel('Timestep', fontsize=12)\n",
        "axes[1].set_ylabel('Hidden Dimension', fontsize=12)\n",
        "axes[1].set_title('Hidden State Heatmap', fontsize=13, fontweight='bold')\n",
        "\n",
        "# Magnitude over time (L2 norm)\n",
        "magnitudes = np.linalg.norm(hidden_long, axis=1)\n",
        "axes[2].plot(magnitudes, linewidth=2.5, marker='o', markersize=5, color='purple')\n",
        "axes[2].set_xlabel('Timestep', fontsize=12)\n",
        "axes[2].set_ylabel('Hidden State Magnitude (L2 Norm)', fontsize=12)\n",
        "axes[2].set_title('Overall Hidden State Magnitude Over Time', fontsize=13, fontweight='bold')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis: Behavior Over Longer Sequences\n",
        "\n",
        "**Key Observations**:\n",
        "\n",
        "1. **Saturation Pattern**:\n",
        "   - Hidden state values tend to saturate (approach tanh's bounds of \u00b11)\n",
        "   - After initial timesteps, changes become smaller\n",
        "   - This is due to tanh's saturation property\n",
        "\n",
        "2. **Magnitude Evolution**:\n",
        "   - The L2 norm shows overall \"activity\" of the hidden state\n",
        "   - Often stabilizes after several timesteps\n",
        "   - This stabilization can be good (stable) or bad (can't adapt to new information)\n",
        "\n",
        "3. **Information Capacity**:\n",
        "   - With only 8 dimensions, the RNN must \"compress\" information from 20 timesteps\n",
        "   - Early information may be forgotten to make room for new information\n",
        "   - This is the fundamental limitation that motivates attention mechanisms!\n",
        "\n",
        "**Why This Matters**:\n",
        "- Real-world sequences are often 100s or 1000s of timesteps (sentences, documents)\n",
        "- RNNs struggle to maintain information over such long sequences\n",
        "- This is why we needed LSTMs (partial solution) and transformers (better solution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1 Summary\n",
        "\n",
        "### Key Insights\n",
        "\n",
        "1. **Hidden Size**:\n",
        "   - Controls representational capacity\n",
        "   - Larger = more capacity but more parameters\n",
        "   - Typical values: 128-1024 for real tasks\n",
        "\n",
        "2. **Activation Functions**:\n",
        "   - **tanh is preferred** for RNNs due to bounded range and symmetry\n",
        "   - ReLU can cause dying neurons and exploding activations\n",
        "   - Bounded activations (tanh, sigmoid) provide stability\n",
        "\n",
        "3. **Sequence Length**:\n",
        "   - RNNs can process arbitrary length sequences\n",
        "   - But information capacity is limited by hidden_size\n",
        "   - Longer sequences \u2192 more challenging to maintain information\n",
        "\n",
        "### Connections to Main Concepts\n",
        "\n",
        "- These experiments show why vanilla RNNs have **limited practical applicability**\n",
        "- The saturation and information loss motivate **LSTMs** (next exercise)\n",
        "- The fundamental capacity limitation motivates **attention mechanisms** (future notebook)\n",
        "\n",
        "**Next**: Let's explore the vanishing gradient problem in depth!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n# Exercise 2: Understand Vanishing Gradients\n\n## Original Exercise\n\n> **Task**: Play with the gradient simulation.\n>\n> Questions:\n> - What weight value keeps gradients stable longest?\n> - Try seq_len=100. Do gradients vanish faster?\n> - Why is it hard to find a weight value that avoids both vanishing AND exploding?\n\n## Learning Objectives\n\n- Find the \"Goldilocks zone\" of weight values\n- Understand the exponential nature of gradient decay\n- See why RNN training is so sensitive to initialization\n\nLet's investigate!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part A: Finding the Stable Weight Value\n\nLet's systematically test different weight values to find which one keeps gradients stable longest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test a range of weight values\nweight_values = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2]\nseq_len = 50\n\n# Store results\ngradient_results = {}\n\nfor w in weight_values:\n    gradients = simulate_gradient_flow(seq_len=seq_len, weight_value=w)\n    gradient_results[w] = gradients\n    # Print gradient at timestep 0 (earliest) - this shows how much information survived\n    print(f\"Weight = {w:.1f}: gradient at t=0 = {gradients[0]:.6e}\")\n\nprint(\"\\n\u2713 Tested weight values from 0.5 to 1.2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize gradient flow for different weights\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# LEFT: Logarithmic scale (shows vanishing clearly)\nax = axes[0]\nfor w in weight_values:\n    ax.semilogy(gradient_results[w], label=f'W={w}', linewidth=2.5, marker='o', markersize=3)\nax.set_xlabel('Timestep (0 = earliest, 49 = latest)', fontsize=12)\nax.set_ylabel('Gradient Magnitude (log scale)', fontsize=12)\nax.set_title('Gradient Flow for Different Weight Values\\n(Logarithmic Scale)', fontsize=13, fontweight='bold')\nax.legend(fontsize=10, ncol=2)\nax.grid(True, alpha=0.4, which='both')\nax.axhline(y=1e-10, color='r', linestyle='--', alpha=0.5, linewidth=2, label='Essentially zero')\n\n# RIGHT: Bar chart showing gradient at timestep 0\nax = axes[1]\ngrad_at_t0 = [gradient_results[w][0] for w in weight_values]\ncolors = ['red' if g < 1e-6 else 'orange' if g < 1e-2 else 'green' if g < 10 else 'red' for g in grad_at_t0]\nax.bar([str(w) for w in weight_values], grad_at_t0, color=colors, alpha=0.7, edgecolor='black')\nax.set_xlabel('Weight Value', fontsize=12)\nax.set_ylabel('Gradient at Timestep 0', fontsize=12)\nax.set_title('Gradient Magnitude at Earliest Timestep\\n(Green=Stable, Orange=Vanishing, Red=Extreme)', fontsize=13, fontweight='bold')\nax.set_yscale('log')\nax.grid(True, alpha=0.3, axis='y')\nax.axhline(y=1, color='k', linestyle='--', alpha=0.5, linewidth=2, label='Gradient = 1 (ideal)')\nax.legend()\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis: The Goldilocks Zone\n\n**Key Findings**:\n\n1. **W = 0.5-0.7** (Red zone - Rapid Vanishing):\n   - Gradients vanish extremely fast\n   - At timestep 0, gradient \u2248 10^-10 (essentially zero)\n   - Cannot learn long-term dependencies at all\n\n2. **W = 0.8-0.9** (Orange zone - Slow Vanishing):\n   - Gradients decay more gradually\n   - W = 0.9 gives gradient \u2248 10^-3 at timestep 0\n   - Best for vanilla RNNs, but still problematic\n\n3. **W = 1.0** (Green zone - Stable!):\n   - Gradient stays close to 1.0 throughout\n   - This is the \"sweet spot\" (barely!)\n   - But very sensitive - hard to achieve in practice\n\n4. **W > 1.0** (Red zone - Exploding):\n   - Gradients explode exponentially\n   - W = 1.2 gives gradient \u2248 10^6 at timestep 0\n   - Training becomes unstable\n\n**The \"Goldilocks Zone\"**: W \u2248 0.9-1.0 works best, but it's extremely narrow!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part B: Longer Sequences (100 Timesteps)\n\nDo gradients vanish faster with longer sequences? Let's find out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare 50 vs 100 timesteps\nseq_lengths = [50, 100]\nweight_test = 0.9  # Use the \"best\" weight from Part A\n\nresults_by_length = {}\n\nfor seq_len in seq_lengths:\n    gradients = simulate_gradient_flow(seq_len=seq_len, weight_value=weight_test)\n    results_by_length[seq_len] = gradients\n    print(f\"Sequence length {seq_len:3d}: gradient at t=0 = {gradients[0]:.6e}\")\n\n# Calculate the ratio\nratio = results_by_length[100][0] / results_by_length[50][0]\nprint(f\"\\nGradient ratio (100-step / 50-step): {ratio:.6e}\")\nprint(f\"This means the gradient is {1/ratio:.1f}x smaller for longer sequences!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the comparison\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# LEFT: Overlaid plots (log scale)\nax = axes[0]\nfor seq_len in seq_lengths:\n    ax.semilogy(results_by_length[seq_len], \n                label=f'{seq_len} timesteps',\n                linewidth=3, marker='o', markersize=4)\nax.set_xlabel('Timestep', fontsize=12)\nax.set_ylabel('Gradient Magnitude (log scale)', fontsize=12)\nax.set_title(f'Gradient Decay: 50 vs 100 Timesteps\\n(Weight = {weight_test})', \n             fontsize=13, fontweight='bold')\nax.legend(fontsize=12)\nax.grid(True, alpha=0.4, which='both')\nax.axhline(y=1e-10, color='r', linestyle='--', alpha=0.5, label='Essentially zero')\n\n# RIGHT: Direct comparison of first 50 timesteps\nax = axes[1]\nax.plot(range(50), results_by_length[50][:50], \n        label='50-step sequence', linewidth=3, marker='o', markersize=4)\nax.plot(range(50), results_by_length[100][:50], \n        label='100-step sequence (first 50)', linewidth=3, marker='s', markersize=4)\nax.set_xlabel('Timestep', fontsize=12)\nax.set_ylabel('Gradient Magnitude (linear scale)', fontsize=12)\nax.set_title('First 50 Timesteps: Same Decay Rate\\n(But 100-step starts from lower base)', \n             fontsize=13, fontweight='bold')\nax.legend(fontsize=12)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis: Exponential Relationship\n\n**Key Finding**: Yes, gradients vanish **much faster** with longer sequences!\n\n**Why**:\n- Each additional timestep multiplies the gradient by approximately (W \u00d7 tanh') \u2248 0.45 (for W=0.9)\n- After 50 steps: gradient \u2248 (0.45)^50 \u2248 10^-3\n- After 100 steps: gradient \u2248 (0.45)^100 \u2248 (10^-3)^2 \u2248 10^-6\n- The relationship is **exponential**, not linear!\n\n**Practical Implication**:\n```\nSequence length   Gradient at t=0   Can learn?\n     10 steps        ~0.1            \u2713 Yes\n     20 steps        ~0.01           \u2713 Maybe\n     50 steps        ~10^-3          \u2717 Difficult\n    100 steps        ~10^-6          \u2717 No\n    500 steps        ~10^-30         \u2717 Impossible\n```\n\n**This is why vanilla RNNs are limited to ~10-20 timesteps in practice!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part C: The Dilemma - Why It's So Hard\n\nWhy can't we just pick the \"right\" weight value to avoid both vanishing and exploding? Let's visualize the problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a detailed analysis of the stability region\nweight_values_fine = np.linspace(0.4, 1.3, 50)  # Finer grid\nseq_len = 50\n\n# For each weight, compute gradient at t=0\nfinal_gradients = []\n\nfor w in weight_values_fine:\n    grads = simulate_gradient_flow(seq_len=seq_len, weight_value=w)\n    final_gradients.append(grads[0])\n\nfinal_gradients = np.array(final_gradients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the stability region\nfig, axes = plt.subplots(2, 1, figsize=(14, 10))\n\n# TOP: Gradient vs Weight (log scale)\nax = axes[0]\nax.semilogy(weight_values_fine, final_gradients, linewidth=3, color='darkblue')\nax.fill_between(weight_values_fine, 1e-10, final_gradients, \n                 where=(final_gradients < 1e-4), alpha=0.3, color='red', label='Vanishing')\nax.fill_between(weight_values_fine, final_gradients, 1e10,\n                 where=(final_gradients > 10), alpha=0.3, color='orange', label='Exploding')\nax.fill_between(weight_values_fine, 1e-10, 1e10,\n                 where=((final_gradients >= 1e-4) & (final_gradients <= 10)), \n                 alpha=0.3, color='green', label='Stable')\nax.axhline(y=1, color='k', linestyle='--', linewidth=2, alpha=0.7, label='Ideal (gradient=1)')\nax.set_xlabel('Weight Value (W)', fontsize=13)\nax.set_ylabel('Gradient at Timestep 0 (log scale)', fontsize=13)\nax.set_title('The Vanishing/Exploding Gradient Dilemma\\n(50 timesteps, tanh activation)', \n             fontsize=14, fontweight='bold')\nax.legend(fontsize=11, loc='upper left')\nax.grid(True, alpha=0.4, which='both')\nax.set_xlim([0.4, 1.3])\n\n# BOTTOM: Zoomed in on the stable region\nax = axes[1]\n# Find stable region\nstable_mask = (final_gradients >= 1e-4) & (final_gradients <= 10)\nstable_weights = weight_values_fine[stable_mask]\nstable_grads = final_gradients[stable_mask]\n\nif len(stable_weights) > 0:\n    ax.plot(weight_values_fine, final_gradients, linewidth=3, color='darkblue')\n    ax.axhline(y=1, color='k', linestyle='--', linewidth=2, alpha=0.7)\n    ax.axhline(y=1e-4, color='r', linestyle='--', linewidth=1.5, alpha=0.5, label='Vanishing threshold')\n    ax.axhline(y=10, color='orange', linestyle='--', linewidth=1.5, alpha=0.5, label='Exploding threshold')\n    ax.fill_between(stable_weights, 0, 20, alpha=0.2, color='green', label='Stable zone')\n    ax.set_ylim([1e-5, 100])\n    ax.set_yscale('log')\n    ax.set_xlabel('Weight Value (W)', fontsize=13)\n    ax.set_ylabel('Gradient (log scale)', fontsize=13)\n    ax.set_title(f'The Narrow \"Goldilocks Zone\": W \u2248 {stable_weights.min():.2f} to {stable_weights.max():.2f}',\n                 fontsize=14, fontweight='bold')\n    ax.legend(fontsize=11)\n    ax.grid(True, alpha=0.4, which='both')\n    ax.set_xlim([0.4, 1.3])\n    \n    # Annotate the stable range\n    stable_range = stable_weights.max() - stable_weights.min()\n    ax.text(0.95, 1.5, f'Stable range width:\\n~{stable_range:.3f}',\n            transform=ax.transData, fontsize=12,\n            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n            ha='center')\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis: Why the Dilemma Exists\n\n**The Problem**: The \"stable zone\" (green) is **extremely narrow**!\n\n**Mathematical Explanation**:\n\n1. **Gradient flow equation**:\n   ```\n   gradient(t=0) = gradient(t=T) \u00d7 \u220f(t=0 to T-1) [W \u00d7 tanh'(...)]\n   ```\n\n2. **For vanilla RNN**:\n   - Each timestep multiplies by: `W \u00d7 tanh'` \u2248 `W \u00d7 0.5`\n   - After T steps: gradient \u2248 `(W \u00d7 0.5)^T`\n\n3. **For gradient to stay near 1**:\n   - Need: `(W \u00d7 0.5)^T \u2248 1`\n   - Solving: `W \u00d7 0.5 \u2248 1` \u2192 `W \u2248 2.0`\n   - But wait... this assumes tanh' is constant!\n\n4. **The reality**:\n   - `tanh'(x)` varies from 0 to 1 depending on x\n   - Average is ~0.5, but actual values fluctuate\n   - So we need W \u2248 0.9-1.0 to compensate\n\n5. **Why it's hard**:\n   ```\n   If W = 0.9:  (0.9 \u00d7 0.5)^50 = 0.45^50 \u2248 10^-3   \u2192 Vanishing\n   If W = 1.0:  (1.0 \u00d7 0.5)^50 = 0.5^50  \u2248 10^-15  \u2192 Still vanishing!\n   If W = 1.1:  (1.1 \u00d7 0.5)^50 = 0.55^50 \u2248 10^-11  \u2192 Still vanishing!\n   If W = 1.5:  (1.5 \u00d7 0.5)^50 = 0.75^50 \u2248 10^-6   \u2192 Exploding!\n   ```\n\n**Why We Can't Just \"Initialize Better\"**:\n- The stable range is ~0.05 wide (e.g., 0.92 to 0.97)\n- Random initialization can't guarantee landing in this range\n- During training, weights will drift out of the stable zone\n- Different layers/dimensions need different optimal weights\n\n**This is THE fundamental problem that LSTMs were designed to solve!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2 Summary\n\n### Key Insights\n\n1. **Optimal Weight Value**: W \u2248 0.9-1.0\n   - This range keeps gradients most stable\n   - But even this is not sufficient for long sequences\n   - The stable zone is extremely narrow (~0.05 wide)\n\n2. **Sequence Length Impact**: Exponential decay\n   - Doubling sequence length doesn't double the problem - it squares it!\n   - 100 timesteps: gradient \u2248 (gradient at 50 steps)\u00b2\n   - Practically limits vanilla RNNs to ~10-20 timesteps\n\n3. **The Vanishing/Exploding Dilemma**:\n   - Too small W \u2192 gradients vanish\n   - Too large W \u2192 gradients explode\n   - \"Just right\" W is nearly impossible to maintain\n   - This is why careful initialization (e.g., Xavier, He) helps but doesn't solve the problem\n\n### Why This Matters\n\n**The vanishing gradient problem is not a bug - it's a fundamental mathematical property of deep recurrent architectures!**\n\nSolutions attempted:\n1. \u2717 **Better initialization**: Helps but doesn't solve it\n2. \u2717 **Gradient clipping**: Prevents exploding but not vanishing\n3. \u2713 **LSTMs/GRUs**: Partially solve with gates and cell state (next exercise!)\n4. \u2713 **Transformers**: Completely avoid the problem with attention\n\n**Next**: Let's see how LSTM gates provide more fine-grained control!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n# Exercise 3: Interpret LSTM Gates\n\n## Original Exercise\n\n> **Task**: Look at the gate visualizations and answer:\n>\n> - Which gate values (0 or 1) indicate \"remembering\" information?\n> - Which indicate \"forgetting\"?\n> - How might gate patterns differ for:\n>   - A sequence that requires long-term memory?\n>   - A sequence where you want to quickly forget old information?\n\n## Learning Objectives\n\n- Understand what each LSTM gate controls\n- Interpret gate activation patterns\n- See how gates adapt to different sequence types\n\nLet's explore LSTM's intelligent memory control!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part A: What Do Gate Values Mean?\n\n### Quick Refresher: The Three Gates\n\n1. **Forget Gate (f_t)**: Controls what to forget from cell state\n   - f_t \u2248 1 \u2192 **Remember** (keep old information)\n   - f_t \u2248 0 \u2192 **Forget** (discard old information)\n\n2. **Input Gate (i_t)**: Controls what new info to add\n   - i_t \u2248 1 \u2192 **Accept** new information\n   - i_t \u2248 0 \u2192 **Ignore** new information\n\n3. **Output Gate (o_t)**: Controls what to output\n   - o_t \u2248 1 \u2192 **Output** this information\n   - o_t \u2248 0 \u2192 **Suppress** this information\n\nLet's see these in action!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LSTM and process a simple sequence\nnp.random.seed(42)\nlstm = LSTMCell(input_size=3, hidden_size=4)\n\n# Process a sequence and track everything\nseq_len = 10\ninputs = np.random.randn(seq_len, 3)\n\n# Initialize\nh = np.zeros(4)\nC = np.zeros(4)\n\n# Storage\nforget_gates = []\ninput_gates = []\noutput_gates = []\ncell_states = []\nhidden_states = []\n\nfor t in range(seq_len):\n    h, C, (f_t, i_t, o_t) = lstm.forward(inputs[t], h, C)\n    \n    forget_gates.append(f_t.copy())\n    input_gates.append(i_t.copy())\n    output_gates.append(o_t.copy())\n    cell_states.append(C.copy())\n    hidden_states.append(h.copy())\n\n# Convert to arrays\nforget_gates = np.array(forget_gates)\ninput_gates = np.array(input_gates)\noutput_gates = np.array(output_gates)\ncell_states = np.array(cell_states)\n\nprint(f\"Processed {seq_len} timesteps through LSTM\")\nprint(f\"Forget gates shape: {forget_gates.shape}\")\nprint(f\"\\nAverage gate values:\")\nprint(f\"  Forget gate: {forget_gates.mean():.3f} (1=remember, 0=forget)\")\nprint(f\"  Input gate:  {input_gates.mean():.3f} (1=accept new, 0=ignore new)\")\nprint(f\"  Output gate: {output_gates.mean():.3f} (1=output, 0=suppress)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize gate patterns\nfig, axes = plt.subplots(2, 2, figsize=(15, 11))\n\n# Forget Gate\nsns.heatmap(forget_gates.T, ax=axes[0, 0], cmap='RdYlGn', vmin=0, vmax=1,\n            xticklabels=range(seq_len), yticklabels=range(4),\n            cbar_kws={'label': 'Gate Value'}, annot=True, fmt='.2f')\naxes[0, 0].set_xlabel('Timestep')\naxes[0, 0].set_ylabel('Hidden Dimension')\naxes[0, 0].set_title('Forget Gate f_t\\n(Green=Remember, Red=Forget)', fontweight='bold')\n\n# Input Gate\nsns.heatmap(input_gates.T, ax=axes[0, 1], cmap='RdYlGn', vmin=0, vmax=1,\n            xticklabels=range(seq_len), yticklabels=range(4),\n            cbar_kws={'label': 'Gate Value'}, annot=True, fmt='.2f')\naxes[0, 1].set_xlabel('Timestep')\naxes[0, 1].set_ylabel('Hidden Dimension')\naxes[0, 1].set_title('Input Gate i_t\\n(Green=Accept New, Red=Ignore)', fontweight='bold')\n\n# Output Gate\nsns.heatmap(output_gates.T, ax=axes[1, 0], cmap='RdYlGn', vmin=0, vmax=1,\n            xticklabels=range(seq_len), yticklabels=range(4),\n            cbar_kws={'label': 'Gate Value'}, annot=True, fmt='.2f')\naxes[1, 0].set_xlabel('Timestep')\naxes[1, 0].set_ylabel('Hidden Dimension')\naxes[1, 0].set_title('Output Gate o_t\\n(Green=Output, Red=Suppress)', fontweight='bold')\n\n# Cell State\nsns.heatmap(cell_states.T, ax=axes[1, 1], cmap='coolwarm', center=0,\n            xticklabels=range(seq_len), yticklabels=range(4),\n            cbar_kws={'label': 'Cell Value'}, annot=True, fmt='.2f')\naxes[1, 1].set_xlabel('Timestep')\naxes[1, 1].set_ylabel('Hidden Dimension')\naxes[1, 1].set_title('Cell State C_t (Long-term Memory)\\n(Red=Positive, Blue=Negative)', fontweight='bold')\n\nplt.suptitle('LSTM Gate Activations Over Time', fontsize=14, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpreting the Patterns\n\n**What We See** (for random input):\n\n1. **Forget Gate** (Top-left):\n   - Values around 0.5 (yellow-green)\n   - This means: \"partially remember, partially forget\"\n   - The LSTM is being cautious - not committing fully\n\n2. **Input Gate** (Top-right):\n   - Also around 0.5\n   - This means: \"partially accept new information\"\n   - Balanced approach to incorporating new inputs\n\n3. **Output Gate** (Bottom-left):\n   - Similar pattern\n   - Controls what gets output to next layer\n\n4. **Cell State** (Bottom-right):\n   - This is the actual long-term memory!\n   - Values can be positive or negative\n   - Notice how it evolves based on forget and input gates\n\n**Key Insight**: For random input, gates hover around 0.5. But for **meaningful sequences**, gates should show clear patterns! Let's test this..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part B: Gate Patterns for Different Sequence Types\n\nLet's create two synthetic sequences with very different requirements and see how a trained LSTM would behave.\n\n### Scenario 1: Long-Term Dependency Task\n**Task**: Remember the first input for all 10 timesteps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scenario 1: Long-term dependency\n# Sequence: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n# Goal: Remember that first \"1\" throughout\n\nnp.random.seed(42)\nlstm_longterm = LSTMCell(input_size=1, hidden_size=4)\n\n# Create sequence: important signal at t=0, then zeros\nseq_longterm = np.zeros((10, 1))\nseq_longterm[0, 0] = 1.0  # Important information at start!\n\n# Process and track\nh = np.zeros(4)\nC = np.zeros(4)\n\nf_gates_lt = []\ni_gates_lt = []\no_gates_lt = []\nC_states_lt = []\n\nfor t in range(len(seq_longterm)):\n    h, C, (f_t, i_t, o_t) = lstm_longterm.forward(seq_longterm[t], h, C)\n    f_gates_lt.append(f_t.copy())\n    i_gates_lt.append(i_t.copy())\n    o_gates_lt.append(o_t.copy())\n    C_states_lt.append(C.copy())\n\nf_gates_lt = np.array(f_gates_lt)\ni_gates_lt = np.array(i_gates_lt)\no_gates_lt = np.array(o_gates_lt)\nC_states_lt = np.array(C_states_lt)\n\nprint(\"Scenario 1: Long-term dependency\")\nprint(f\"  Average forget gate after t=0: {f_gates_lt[1:].mean():.3f}\")\nprint(f\"  Average input gate after t=0: {i_gates_lt[1:].mean():.3f}\")\nprint(\"  \u2192 High forget gate would mean 'keep remembering the initial 1'\")\nprint(\"  \u2192 Low input gate would mean 'ignore the zeros that follow'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scenario 2: Rapidly Changing Task\n**Task**: Track a counting sequence where each new number matters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scenario 2: Rapidly changing\n# Sequence: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n# Goal: Keep updating with new information\n\nnp.random.seed(42)\nlstm_rapid = LSTMCell(input_size=1, hidden_size=4)\n\n# Create sequence: constantly changing\nseq_rapid = np.arange(1, 11).reshape(-1, 1).astype(float)\n\n# Process and track\nh = np.zeros(4)\nC = np.zeros(4)\n\nf_gates_rp = []\ni_gates_rp = []\no_gates_rp = []\nC_states_rp = []\n\nfor t in range(len(seq_rapid)):\n    h, C, (f_t, i_t, o_t) = lstm_rapid.forward(seq_rapid[t], h, C)\n    f_gates_rp.append(f_t.copy())\n    i_gates_rp.append(i_t.copy())\n    o_gates_rp.append(o_t.copy())\n    C_states_rp.append(C.copy())\n\nf_gates_rp = np.array(f_gates_rp)\ni_gates_rp = np.array(i_gates_rp)\no_gates_rp = np.array(o_gates_rp)\nC_states_rp = np.array(C_states_rp)\n\nprint(\"Scenario 2: Rapidly changing\")\nprint(f\"  Average forget gate: {f_gates_rp.mean():.3f}\")\nprint(f\"  Average input gate: {i_gates_rp.mean():.3f}\")\nprint(\"  \u2192 Ideally: low forget gate (don't hold onto old) + high input gate (accept new)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare gate patterns side-by-side\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\n\nscenarios = [\n    ('Long-term Dependency\\n(Remember first input)', f_gates_lt, i_gates_lt, o_gates_lt),\n    ('Rapidly Changing\\n(Update constantly)', f_gates_rp, i_gates_rp, o_gates_rp)\n]\n\nfor row, (title, f_g, i_g, o_g) in enumerate(scenarios):\n    # Forget gate\n    sns.heatmap(f_g.T, ax=axes[row, 0], cmap='RdYlGn', vmin=0, vmax=1,\n                xticklabels=range(10), yticklabels=range(4),\n                cbar_kws={'label': 'Value'}, annot=True, fmt='.2f')\n    axes[row, 0].set_xlabel('Timestep')\n    axes[row, 0].set_ylabel('Dimension')\n    axes[row, 0].set_title(f'Forget Gate\\n{title}', fontsize=11, fontweight='bold')\n    \n    # Input gate\n    sns.heatmap(i_g.T, ax=axes[row, 1], cmap='RdYlGn', vmin=0, vmax=1,\n                xticklabels=range(10), yticklabels=range(4),\n                cbar_kws={'label': 'Value'}, annot=True, fmt='.2f')\n    axes[row, 1].set_xlabel('Timestep')\n    axes[row, 1].set_ylabel('Dimension')\n    axes[row, 1].set_title(f'Input Gate\\n{title}', fontsize=11, fontweight='bold')\n    \n    # Output gate\n    sns.heatmap(o_g.T, ax=axes[row, 2], cmap='RdYlGn', vmin=0, vmax=1,\n                xticklabels=range(10), yticklabels=range(4),\n                cbar_kws={'label': 'Value'}, annot=True, fmt='.2f')\n    axes[row, 2].set_xlabel('Timestep')\n    axes[row, 2].set_ylabel('Dimension')\n    axes[row, 2].set_title(f'Output Gate\\n{title}', fontsize=11, fontweight='bold')\n\nplt.suptitle('LSTM Gate Patterns for Different Sequence Types', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis: What Should We Expect?\n\n**Note**: The LSTM above is **untrained** (random weights), so we don't see ideal patterns yet. But let's discuss what a **trained** LSTM would learn:\n\n#### For Long-term Dependency Task:\n**Ideal behavior**:\n1. **t=0** (seeing the important \"1\"):\n   - High input gate (i_t \u2248 1): \"Accept this important information!\"\n   - Forget gate can be low: \"Nothing to remember from before\"\n\n2. **t=1 to 9** (seeing zeros):\n   - **High forget gate** (f_t \u2248 1): \"Keep remembering what I stored at t=0!\"\n   - **Low input gate** (i_t \u2248 0): \"Ignore these meaningless zeros\"\n   - Output gate: depends on task\n\n**Why this works**: Cell state equation is `C_t = f_t * C_{t-1} + i_t * C_tilde`\n- If f_t \u2248 1 and i_t \u2248 0: `C_t \u2248 C_{t-1}` \u2192 **Memory is preserved!**\n\n#### For Rapidly Changing Task:\n**Ideal behavior**:\n1. **Every timestep**:\n   - **Moderate forget gate** (f_t \u2248 0.3-0.7): \"Partially forget old count\"\n   - **High input gate** (i_t \u2248 0.7-1.0): \"Strongly accept new count\"\n   - This allows the model to update quickly\n\n**Why this works**:\n- If f_t \u2248 0.5 and i_t \u2248 0.8: `C_t \u2248 0.5 * C_{t-1} + 0.8 * new_info`\n- Old information fades, new information dominates\n\n### Key Insight\n\n**LSTMs learn to modulate their gates based on the task!**\n- Long-term tasks \u2192 high forget gates (remember)\n- Short-term tasks \u2192 low forget gates (quickly update)\n- The gates provide **fine-grained control** over information flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3 Summary\n\n### Key Insights\n\n1. **Gate Value Meanings**:\n   - **Forget gate**: 1=remember, 0=forget\n   - **Input gate**: 1=accept new info, 0=ignore new info\n   - **Output gate**: 1=output this, 0=suppress\n\n2. **Task-Dependent Patterns**:\n   - **Long-term memory tasks**: High forget gates + low input gates\n     - Preserves information over many timesteps\n     - `C_t \u2248 C_0` (cell state stays constant)\n   \n   - **Rapidly changing tasks**: Low forget gates + high input gates\n     - Quickly updates to new information\n     - `C_t \u2248 new_info` (cell state tracks current input)\n\n3. **Fine-Grained Control**:\n   - Unlike vanilla RNN with single hidden state\n   - LSTM has 3 independent gates per dimension\n   - Can simultaneously: remember some dims, forget others, update others\n   - This is why LSTMs handle ~50-100 timesteps vs RNN's ~10-20\n\n### Why This Matters\n\n**LSTMs provide adaptive memory management**:\n- Vanilla RNN: Fixed update rule `h_t = tanh(W @ h_{t-1} + ...)`\n- LSTM: Learned gates decide what to remember/forget/update\n- This flexibility enables longer-term dependencies\n\n**But LSTMs still have limitations**:\n- Gates help but don't fully solve vanishing gradients\n- Still sequential (can't parallelize)\n- Still have information bottleneck (fixed hidden size)\n\nThis motivates **attention mechanisms** and **transformers**!\n\n**Next**: Let's think conceptually about how to solve these remaining problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n# Exercise 4: Think Ahead to Transformers\n\n## Original Exercise\n\n> **Task**: Before seeing the next notebook, brainstorm:\n>\n> - How could we give a model \"direct access\" to all inputs at once?\n> - How might we process an entire sequence in parallel?\n> - Sketch your ideas on paper!\n\n## Learning Objectives\n\n- Think creatively about the problems we've identified\n- Understand the conceptual gaps that transformers fill\n- Prepare mentally for the attention mechanism\n\nThis is a **conceptual exercise** - there's no single right answer! The goal is to think about the problems from first principles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Review: What Problems Do We Need to Solve?\n\nFrom our previous exercises, we've identified these fundamental limitations of RNNs/LSTMs:\n\n### Problem 1: Sequential Processing\n```\nRNN/LSTM:\n  t=0 \u2192 t=1 \u2192 t=2 \u2192 t=3 \u2192 t=4 \u2192 t=5\n  (must wait for each step before proceeding)\n\n\u274c Slow: Can't use modern GPUs effectively\n\u274c Serial: 100-word sentence = 100 sequential steps\n```\n\n### Problem 2: No Direct Access\n```\nTo connect word at position 1 to word at position 50:\n  Information must flow through 49 intermediate steps\n  Each step degrades the signal (even with LSTMs)\n\n\u274c Indirect path: Information gets corrupted\n\u274c Long-range: Harder as distance increases\n```\n\n### Problem 3: Fixed-Size Bottleneck\n```\nIn seq2seq:\n  100-word input \u2192 compress to fixed vector \u2192 generate output\n  \n\u274c Information loss: Can't fit everything in fixed size\n\u274c Length-dependent: Quality degrades with longer inputs\n```\n\n### Problem 4: Limited Attention\n```\nWhen processing word 50, RNN has:\n  - Strong memory of words 45-50 (recent)\n  - Weak memory of words 1-10 (distant)\n  - No way to \"look back\" directly\n\n\u274c Recency bias: Recent words dominate\n\u274c No selective focus: Can't choose what to focus on\n```\n\nLet's brainstorm solutions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Brainstorm 1: How to Give \"Direct Access\" to All Inputs?\n\n### Idea 1: Random Access Memory\n```\nConcept: Store all inputs in a memory bank with index-based lookup\n\nStructure:\n  Memory = [word_1, word_2, word_3, ..., word_N]\n  \nAt any timestep t, can access any word_i directly:\n  - No need to propagate through intermediate steps\n  - Like RAM in a computer: O(1) access time\n\nExample:\n  Processing word_50? \n  Can directly look at word_1 without going through word_2...word_49!\n\nPros:\n  \u2713 Direct access - no information degradation\n  \u2713 No distance problem\n  \nCons:\n  \u2717 How to decide WHICH words to access?\n  \u2717 How to combine information from multiple words?\n  \u2717 Still sequential if we process one word at a time\n```\n\n### Idea 2: Fully Connected Graph\n```\nConcept: Connect every word to every other word\n\nStructure:\n  word_1 \u2190\u2192 word_2\n  word_1 \u2190\u2192 word_3\n  word_1 \u2190\u2192 word_4\n  ...\n  word_2 \u2190\u2192 word_3\n  word_2 \u2190\u2192 word_4\n  ...\n  (Every word connected to every other word!)\n\nExample:\n  word_1 can directly influence word_50\n  word_50 can directly influence word_1\n  \nPros:\n  \u2713 Every word can \"see\" every other word\n  \u2713 No distance limitation\n  \u2713 Information flows in all directions\n\nCons:\n  \u2717 Massive number of connections: N^2 for N words\n  \u2717 How to weight/combine all these connections?\n  \u2717 Still need some way to aggregate information\n```\n\n### Idea 3: Attention-Based Lookup\n```\nConcept: When processing word_i, compute a \"relevance score\" for all other words\n\nIntuition:\n  \"Which other words should I pay attention to?\"\n  \nExample - Translating \"The cat sat on the mat\":\n  Processing \"cat\":\n    - High relevance to \"The\" (subject marker)\n    - High relevance to \"sat\" (verb, what did cat do?)\n    - Low relevance to \"on\" (preposition, less important)\n    - Medium relevance to \"mat\" (location)\n\nMechanism:\n  For each word:\n    1. Compute relevance scores to ALL other words\n    2. Weight other words by relevance\n    3. Combine weighted information\n\nPros:\n  \u2713 Direct access to all words\n  \u2713 Learned relevance (not fixed)\n  \u2713 Can focus on important words, ignore irrelevant ones\n\nCons:\n  \u2717 How to compute \"relevance\"?\n  \u2717 Still seems complex...\n\n\ud83d\udca1 This is basically the ATTENTION MECHANISM!\n```\n\n### Your Ideas?\n\nBefore moving forward, think:\n- Which of these ideas seems most promising?\n- Can you think of other approaches?\n- What are the key challenges to solve?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Brainstorm 2: How to Process Sequences in Parallel?\n\n### Current Problem: Sequential Dependency\n```\nRNN/LSTM:\n  h_1 = f(x_1, h_0)      \u2190 depends on h_0\n  h_2 = f(x_2, h_1)      \u2190 depends on h_1 \u2190 can't compute until h_1 done\n  h_3 = f(x_3, h_2)      \u2190 depends on h_2 \u2190 can't compute until h_2 done\n  ...\n\nThis is INHERENTLY SEQUENTIAL.\nCan't compute h_3 until h_2 is done.\n```\n\n### Idea 1: Remove Recurrence Entirely\n```\nConcept: Don't use previous hidden states at all!\n\nInstead:\n  h_1 = f(x_1, ALL_OTHER_WORDS)\n  h_2 = f(x_2, ALL_OTHER_WORDS)\n  h_3 = f(x_3, ALL_OTHER_WORDS)\n  ...\n\nKey insight:\n  Each h_i can be computed INDEPENDENTLY!\n  No dependency on h_{i-1}\n  \nParallelization:\n  Compute all h_i simultaneously using matrix operations\n  \nExample (for 4 words):\n  h_1, h_2, h_3, h_4 = parallel_function([x_1, x_2, x_3, x_4])\n  \nPros:\n  \u2713 Fully parallel - no waiting\n  \u2713 Can leverage GPUs (great at parallel matrix ops)\n  \u2713 Faster training and inference\n\nCons:\n  \u2717 How does each word know about context?\n  \u2717 Isn't the \"memory\" important?\n  \u2192 Solution: Let each word look at ALL other words! (attention again!)\n```\n\n### Idea 2: Matrix Operations Instead of Loops\n```\nConcept: Replace for-loop over timesteps with matrix multiplication\n\nCurrent (sequential):\n  for t in range(seq_len):\n      h_t = process(x_t, h_{t-1})\n\nNew (parallel):\n  H = matrix_operation(X)  # Process entire sequence at once!\n  \nWhere:\n  X = [x_1, x_2, ..., x_N]  (entire input sequence)\n  H = [h_1, h_2, ..., h_N]  (entire output sequence)\n  \nMatrix operations are highly parallelizable on GPUs!\n\nExample:\n  Instead of:\n    for i in range(1000):  # 1000 sequential steps\n        result[i] = compute(input[i])\n  \n  Do:\n    result = matrix_multiply(weights, input)  # Single parallel operation!\n```\n\n### Idea 3: Position-Aware Processing\n```\nProblem: If we remove recurrence, how does the model know word order?\n  \"cat dog bit\" vs \"dog cat bit\" should be different!\n  But if processing in parallel, how to encode position?\n\nSolution: Add position information to each word!\n\nConcept:\n  word_embedding = content_embedding + position_embedding\n  \nExample:\n  \"cat\" at position 1: [cat_vector] + [position_1_vector]\n  \"cat\" at position 5: [cat_vector] + [position_5_vector]\n  \nNow each word knows:\n  - What it is (content)\n  - Where it is (position)\n  \nThis allows parallel processing while preserving order!\n\n\ud83d\udca1 This is POSITIONAL ENCODING in transformers!\n```\n\n### The Big Insight\n\n**Transformers combine these ideas**:\n1. Remove recurrence \u2192 enable parallelization\n2. Add attention \u2192 give direct access to all words\n3. Add positional encodings \u2192 preserve order information\n\nResult:\n- \u2713 Parallel processing (fast!)\n- \u2713 Direct connections (no degradation!)\n- \u2713 Position-aware (understands order!)\n- \u2713 Selective focus (attention to relevant words!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization: RNN vs Transformer Architecture\n\n### RNN/LSTM Architecture\n```\nSequential Processing:\n\nTime:    t=0      t=1      t=2      t=3\n         \u2193        \u2193        \u2193        \u2193\nInput:  [The]  \u2192 [cat]  \u2192 [sat]  \u2192 [down]\n         \u2193        \u2193        \u2193        \u2193  \nRNN:    h_0 --\u2192 h_1 --\u2192 h_2 --\u2192 h_3\n         \u2193        \u2193        \u2193        \u2193\nOutput: y_0     y_1      y_2      y_3\n\nInformation flow:\n- \"The\" must pass through h_0 \u2192 h_1 \u2192 h_2 \u2192 h_3 to reach \"down\"\n- Sequential: Can't start h_2 until h_1 finishes\n- Distance problem: Information from \"The\" degrades by time it reaches \"down\"\n```\n\n### Transformer Architecture (Preview)\n```\nParallel Processing:\n\nInput:  [The]    [cat]    [sat]    [down]\n         \u2193        \u2193        \u2193        \u2193\n      Encode   Encode   Encode   Encode  (Add position info)\n         \u2193        \u2193        \u2193        \u2193\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2193\n         Self-Attention Layer\n         (Every word looks at every word)\n                  \u2193\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2193        \u2193        \u2193        \u2193\n       out_0    out_1    out_2    out_3\n\nInformation flow:\n- \"The\" can DIRECTLY access \"down\" (and vice versa)\n- Parallel: All words processed simultaneously\n- No distance problem: Direct connections everywhere\n- Attention: Each word decides which others are relevant\n```\n\n### Key Differences\n\n| Aspect | RNN/LSTM | Transformer |\n|--------|----------|-------------|\n| **Processing** | Sequential | Parallel |\n| **Access** | Indirect (through chain) | Direct (all-to-all) |\n| **Speed** | Slow (must wait) | Fast (parallel) |\n| **Long-range** | Difficult (degrades) | Easy (direct) |\n| **Bottleneck** | Fixed hidden size | Attention to all words |\n| **Position** | Implicit (order of processing) | Explicit (positional encoding) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4 Summary\n\n### Key Conceptual Insights\n\n1. **Direct Access Problem**:\n   - RNNs force indirect access through sequential chain\n   - Solution ideas: memory bank, fully connected graph, **attention mechanism**\n   - Attention lets each word compute relevance to all others\n\n2. **Parallel Processing Problem**:\n   - Recurrence creates sequential dependency\n   - Solution: Remove recurrence, use matrix operations\n   - Add positional encodings to preserve order\n\n3. **The Transformer Approach**:\n   - No recurrence \u2192 parallelizable\n   - Attention \u2192 direct access\n   - Positional encoding \u2192 order-aware\n   - Result: Fast, powerful, scalable\n\n### Why This Mental Model Matters\n\nWhen you learn about transformers in the next notebook:\n- **Self-attention** will seem natural (we brainstormed this!)\n- **Positional encoding** will make sense (needed for parallel processing)\n- **Multi-head attention** will be a logical extension\n\nYou now understand **WHY** transformers were invented:\n1. RNNs are slow (sequential)\n2. RNNs struggle with long sequences (vanishing gradients)\n3. RNNs have information bottlenecks (fixed hidden size)\n4. We need: parallel + direct access + selective focus\n\n### Preparing for the Next Notebook\n\nIn `02_attention_mechanism.ipynb`, you'll learn:\n- How attention computes \"relevance scores\"\n- What Query, Key, and Value mean\n- How self-attention enables all-to-all connections\n- How to implement attention from scratch\n- Why \"Attention Is All You Need\"\n\nYou're now mentally prepared for one of the most important ideas in modern AI! \ud83d\ude80\n\n**Next**: Before we move to transformers, let's build something practical - a character-level language model with full backpropagation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n# Exercise 5: Build a Character-Level Language Model\n\n## Original Exercise\n\n> **Task (Advanced)**: Implement a character-level language model.\n>\n> - Train an RNN to predict the next character\n> - Given: \"hell\" \u2192 predict: \"o\"\n> - Given: \"hello worl\" \u2192 predict: \"d\"\n>\n> **Hints**:\n> - Use the `SimpleRNN` class we built\n> - Start with a very short text (\"hello world\")\n> - One-hot encode characters\n> - See how well it learns!\n\n## What We'll Build\n\nWe'll create a **complete working language model** with:\n1. **Data preparation**: Vocabulary, one-hot encoding, training pairs\n2. **Architecture**: CharRNN with output layer for prediction\n3. **Backpropagation**: Full BPTT (Backpropagation Through Time) implementation\n4. **Training**: Gradient descent with loss tracking\n5. **Evaluation**: Text generation and analysis\n\nThis is challenging but incredibly rewarding - you'll implement the core algorithm behind modern LLMs!\n\nLet's build it step by step. \ud83d\ude80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Data Preparation\n\nFirst, let's prepare our training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training text: Start simple!\ntraining_text = \"hello world\"\n\nprint(f\"Training text: '{training_text}'\")\nprint(f\"Length: {len(training_text)} characters\")\n\n# Build vocabulary\nchars = sorted(list(set(training_text)))\nvocab_size = len(chars)\n\nprint(f\"\\nVocabulary ({vocab_size} unique characters):\")\nprint(chars)\n\n# Create character mappings\nchar_to_idx = {ch: i for i, ch in enumerate(chars)}\nidx_to_char = {i: ch for i, ch in enumerate(chars)}\n\nprint(f\"\\nCharacter \u2192 Index mapping:\")\nfor ch, idx in char_to_idx.items():\n    print(f\"  '{ch}' \u2192 {idx}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### One-Hot Encoding\n\nWe need to convert characters to numerical vectors that the RNN can process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def char_to_onehot(char, char_to_idx, vocab_size):\n    \"\"\"\n    Convert a character to a one-hot encoded vector.\n    \n    Args:\n        char: Character to encode\n        char_to_idx: Dictionary mapping characters to indices\n        vocab_size: Size of vocabulary\n    \n    Returns:\n        One-hot vector of shape (vocab_size,)\n    \"\"\"\n    vec = np.zeros(vocab_size)\n    vec[char_to_idx[char]] = 1.0\n    return vec\n\ndef onehot_to_char(onehot, idx_to_char):\n    \"\"\"\n    Convert a one-hot vector (or probability distribution) to a character.\n    \n    Args:\n        onehot: One-hot vector or probability distribution\n        idx_to_char: Dictionary mapping indices to characters\n    \n    Returns:\n        Character corresponding to the highest probability\n    \"\"\"\n    idx = np.argmax(onehot)\n    return idx_to_char[idx]\n\n# Test encoding\ntest_char = 'h'\ntest_vec = char_to_onehot(test_char, char_to_idx, vocab_size)\n\nprint(f\"One-hot encoding of '{test_char}':\")\nprint(test_vec)\nprint(f\"\\nDecoding back: '{onehot_to_char(test_vec, idx_to_char)}'\")\nprint(\"\\n\u2713 One-hot encoding works!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Training Pairs\n\nWe'll create sliding windows: given a sequence of characters, predict the next one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_training_data(text, char_to_idx, vocab_size, seq_length=3):\n    \"\"\"\n    Create input-target pairs for training.\n    \n    Args:\n        text: Training text\n        char_to_idx: Character to index mapping\n        vocab_size: Size of vocabulary\n        seq_length: Length of input sequences\n    \n    Returns:\n        inputs: List of input sequences (each seq_length characters)\n        targets: List of target characters (next character after sequence)\n    \"\"\"\n    inputs = []\n    targets = []\n    \n    # Sliding window over text\n    for i in range(len(text) - seq_length):\n        # Input: seq_length characters\n        input_seq = text[i:i+seq_length]\n        # Target: next character\n        target_char = text[i+seq_length]\n        \n        # Convert to one-hot\n        input_onehot = np.array([char_to_onehot(ch, char_to_idx, vocab_size) \n                                  for ch in input_seq])\n        target_idx = char_to_idx[target_char]\n        \n        inputs.append(input_onehot)\n        targets.append(target_idx)\n    \n    return inputs, targets\n\n# Create training data with sequence length 3\nseq_length = 3\ninputs, targets = create_training_data(training_text, char_to_idx, vocab_size, seq_length)\n\nprint(f\"Created {len(inputs)} training pairs with sequence length {seq_length}\\n\")\nprint(\"Examples:\")\nfor i in range(min(5, len(inputs))):\n    # Decode input sequence\n    input_chars = ''.join([onehot_to_char(inputs[i][j], idx_to_char) \n                           for j in range(seq_length)])\n    target_char = idx_to_char[targets[i]]\n    print(f\"  Input: '{input_chars}' \u2192 Target: '{target_char}'\")\n\nprint(f\"\\n\u2713 Training data ready: {len(inputs)} examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: CharRNN Architecture\n\nNow let's build our character-level RNN with an output layer for predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CharRNN:\n    \"\"\"\n    Character-level RNN for language modeling.\n    \n    Architecture:\n      Input (one-hot) \u2192 RNN \u2192 Output layer \u2192 Predictions\n    \n    Implements full backpropagation through time (BPTT).\n    \"\"\"\n    \n    def __init__(self, vocab_size, hidden_size, learning_rate=0.01):\n        \"\"\"\n        Initialize CharRNN.\n        \n        Args:\n            vocab_size: Number of unique characters\n            hidden_size: Size of hidden state\n            learning_rate: Learning rate for gradient descent\n        \"\"\"\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.learning_rate = learning_rate\n        \n        # RNN weights (input to hidden, hidden to hidden)\n        self.W_xh = np.random.randn(vocab_size, hidden_size) * 0.01\n        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n        self.b_h = np.zeros(hidden_size)\n        \n        # Output weights (hidden to output)\n        self.W_hy = np.random.randn(hidden_size, vocab_size) * 0.01\n        self.b_y = np.zeros(vocab_size)\n        \n        print(f\"Initialized CharRNN:\")\n        print(f\"  Vocabulary size: {vocab_size}\")\n        print(f\"  Hidden size: {hidden_size}\")\n        print(f\"  Parameters: {self._count_parameters()}\")\n    \n    def _count_parameters(self):\n        \"\"\"Count total number of parameters.\"\"\"\n        return (self.W_xh.size + self.W_hh.size + self.b_h.size + \n                self.W_hy.size + self.b_y.size)\n    \n    def forward(self, inputs):\n        \"\"\"\n        Forward pass through the RNN.\n        \n        Args:\n            inputs: Input sequence, shape (seq_len, vocab_size)\n        \n        Returns:\n            outputs: Output logits, shape (seq_len, vocab_size)\n            hidden_states: Hidden states, shape (seq_len, hidden_size)\n        \"\"\"\n        seq_len = len(inputs)\n        hidden_states = []\n        outputs = []\n        \n        # Initial hidden state\n        h = np.zeros(self.hidden_size)\n        \n        for t in range(seq_len):\n            # RNN update\n            h = np.tanh(inputs[t] @ self.W_xh + h @ self.W_hh + self.b_h)\n            \n            # Output layer\n            y = h @ self.W_hy + self.b_y\n            \n            hidden_states.append(h.copy())\n            outputs.append(y)\n        \n        return np.array(outputs), np.array(hidden_states)\n    \n    def predict(self, inputs):\n        \"\"\"\n        Predict next character probabilities.\n        \n        Args:\n            inputs: Input sequence, shape (seq_len, vocab_size)\n        \n        Returns:\n            Probability distribution over vocabulary for last timestep\n        \"\"\"\n        outputs, _ = self.forward(inputs)\n        last_output = outputs[-1]\n        return self._softmax(last_output)\n    \n    def _softmax(self, x):\n        \"\"\"Numerically stable softmax.\"\"\"\n        exp_x = np.exp(x - np.max(x))\n        return exp_x / exp_x.sum()\n    \n# Create model\nhidden_size = 10\nmodel = CharRNN(vocab_size, hidden_size, learning_rate=0.1)\n\nprint(\"\\n\u2713 CharRNN model created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Forward Pass\n\nLet's verify the model can process input and make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with first training example\ntest_input = inputs[0]  # \"hel\"\ntest_target = targets[0]  # expects 'l'\n\nprint(f\"Input sequence: {test_input.shape}\")\nprint(f\"Target: {idx_to_char[test_target]}\")\n\n# Forward pass\noutputs, hiddens = model.forward(test_input)\n\nprint(f\"\\nOutputs shape: {outputs.shape}\")\nprint(f\"Hidden states shape: {hiddens.shape}\")\n\n# Get prediction\nprobs = model.predict(test_input)\npredicted_char = idx_to_char[np.argmax(probs)]\n\nprint(f\"\\nPrediction probabilities:\")\nfor i, ch in enumerate(chars):\n    print(f\"  '{ch}': {probs[i]:.3f}\")\n\nprint(f\"\\nPredicted character: '{predicted_char}'\")\nprint(f\"Actual target: '{idx_to_char[test_target]}'\")\nprint(\"\\n(Random weights, so prediction is random - we'll train it next!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Loss Function\n\nWe'll use cross-entropy loss to measure how well the model predicts the target character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cross_entropy_loss(probs, target_idx):\n    \"\"\"\n    Compute cross-entropy loss.\n    \n    Args:\n        probs: Probability distribution over vocabulary\n        target_idx: Index of correct character\n    \n    Returns:\n        Loss value (scalar)\n    \"\"\"\n    # Cross-entropy: -log(probability of correct answer)\n    return -np.log(probs[target_idx] + 1e-8)  # Add small epsilon to avoid log(0)\n\n# Test loss\ntest_loss = cross_entropy_loss(probs, test_target)\nprint(f\"Loss for random prediction: {test_loss:.3f}\")\nprint(f\"Random baseline loss: {-np.log(1/vocab_size):.3f}\")\nprint(\"\\n(Lower is better. Random guessing gives loss \u2248 {:.3f})\".format(-np.log(1/vocab_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Backpropagation Through Time (BPTT)\n\nThis is the core algorithm! We'll manually compute gradients through time.\n\n### Mathematical Background\n\n**Forward pass equations**:\n```\nh_t = tanh(W_xh @ x_t + W_hh @ h_{t-1} + b_h)\ny_t = W_hy @ h_t + b_y\nloss = -log(softmax(y_T)[target])\n```\n\n**Backward pass** (chain rule):\n```\n1. Output layer gradients:\n   dL/dy_T = softmax(y_T) - target_onehot\n   dL/dW_hy = h_T^T @ dL/dy_T\n   dL/db_y = dL/dy_T\n\n2. Hidden state gradients (flow backward through time):\n   dL/dh_T = dL/dy_T @ W_hy^T\n   dL/dh_t = dL/dh_{t+1} @ W_hh^T * tanh'(h_t)  (for t < T)\n\n3. Weight gradients (accumulate over time):\n   dL/dW_hh = sum over t: dL/dh_t^T @ h_{t-1}\n   dL/dW_xh = sum over t: dL/dh_t^T @ x_t\n```\n\nLet's implement this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def backward_pass(model, inputs, target_idx, outputs, hidden_states):\n    \"\"\"\n    Backpropagation through time (BPTT).\n    \n    Args:\n        model: CharRNN model\n        inputs: Input sequence, shape (seq_len, vocab_size)\n        target_idx: Index of target character\n        outputs: Forward pass outputs, shape (seq_len, vocab_size)\n        hidden_states: Forward pass hidden states, shape (seq_len, hidden_size)\n    \n    Returns:\n        Dictionary of gradients for all parameters\n    \"\"\"\n    seq_len = len(inputs)\n    \n    # Initialize gradient accumulators\n    dW_xh = np.zeros_like(model.W_xh)\n    dW_hh = np.zeros_like(model.W_hh)\n    db_h = np.zeros_like(model.b_h)\n    dW_hy = np.zeros_like(model.W_hy)\n    db_y = np.zeros_like(model.b_y)\n    \n    # ==== STEP 1: Output layer gradients ====\n    # We only compute loss at the final timestep\n    final_output = outputs[-1]\n    probs = model._softmax(final_output)\n    \n    # Gradient of loss w.r.t. final output (softmax derivative)\n    dy = probs.copy()\n    dy[target_idx] -= 1.0  # Derivative of cross-entropy with softmax\n    \n    # Gradient w.r.t. output weights\n    final_hidden = hidden_states[-1]\n    dW_hy = np.outer(final_hidden, dy)\n    db_y = dy\n    \n    # ==== STEP 2: Backpropagate to final hidden state ====\n    dh_next = dy @ model.W_hy.T\n    \n    # ==== STEP 3: Backpropagate through time ====\n    for t in reversed(range(seq_len)):\n        # Current hidden state\n        h_t = hidden_states[t]\n        \n        # Gradient of tanh activation\n        # tanh'(x) = 1 - tanh^2(x)\n        dtanh = 1 - h_t**2\n        \n        # Gradient before activation\n        dh_raw = dh_next * dtanh\n        \n        # Accumulate weight gradients\n        if t > 0:\n            h_prev = hidden_states[t-1]\n        else:\n            h_prev = np.zeros(model.hidden_size)\n        \n        dW_hh += np.outer(h_prev, dh_raw)\n        dW_xh += np.outer(inputs[t], dh_raw)\n        db_h += dh_raw\n        \n        # Gradient for previous hidden state\n        dh_next = dh_raw @ model.W_hh.T\n    \n    return {\n        'W_xh': dW_xh,\n        'W_hh': dW_hh,\n        'b_h': db_h,\n        'W_hy': dW_hy,\n        'b_y': db_y\n    }\n\nprint(\"\u2713 BPTT implementation ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gradient Clipping\n\nTo prevent exploding gradients, we'll clip gradients to a maximum norm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clip_gradients(gradients, max_norm=5.0):\n    \"\"\"\n    Clip gradients to prevent explosion.\n    \n    Args:\n        gradients: Dictionary of gradients\n        max_norm: Maximum gradient norm\n    \n    Returns:\n        Clipped gradients\n    \"\"\"\n    # Compute total norm\n    total_norm = 0\n    for grad in gradients.values():\n        total_norm += np.sum(grad**2)\n    total_norm = np.sqrt(total_norm)\n    \n    # Clip if necessary\n    if total_norm > max_norm:\n        scale = max_norm / total_norm\n        for key in gradients:\n            gradients[key] *= scale\n    \n    return gradients, total_norm\n\nprint(\"\u2713 Gradient clipping ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Training Loop\n\nNow let's put it all together and train the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_charrnn(model, inputs, targets, epochs=100, print_every=10):\n    \"\"\"\n    Train the CharRNN model.\n    \n    Args:\n        model: CharRNN model\n        inputs: List of input sequences\n        targets: List of target character indices\n        epochs: Number of training epochs\n        print_every: Print progress every N epochs\n    \n    Returns:\n        List of losses over training\n    \"\"\"\n    losses = []\n    \n    for epoch in range(epochs):\n        epoch_loss = 0\n        \n        # Train on each example\n        for input_seq, target_idx in zip(inputs, targets):\n            # Forward pass\n            outputs, hidden_states = model.forward(input_seq)\n            \n            # Compute loss\n            probs = model._softmax(outputs[-1])\n            loss = cross_entropy_loss(probs, target_idx)\n            epoch_loss += loss\n            \n            # Backward pass\n            grads = backward_pass(model, input_seq, target_idx, outputs, hidden_states)\n            \n            # Clip gradients\n            grads, grad_norm = clip_gradients(grads, max_norm=5.0)\n            \n            # Update weights (gradient descent)\n            model.W_xh -= model.learning_rate * grads['W_xh']\n            model.W_hh -= model.learning_rate * grads['W_hh']\n            model.b_h -= model.learning_rate * grads['b_h']\n            model.W_hy -= model.learning_rate * grads['W_hy']\n            model.b_y -= model.learning_rate * grads['b_y']\n        \n        # Average loss for epoch\n        avg_loss = epoch_loss / len(inputs)\n        losses.append(avg_loss)\n        \n        # Print progress\n        if (epoch + 1) % print_every == 0:\n            print(f\"Epoch {epoch+1:3d}/{epochs}: Loss = {avg_loss:.4f}\")\n    \n    return losses\n\nprint(\"\u2713 Training function ready!\")\nprint(\"\\nStarting training...\")\nprint(\"=\"*50)\n\n# Train the model!\nlosses = train_charrnn(model, inputs, targets, epochs=200, print_every=20)\n\nprint(\"=\"*50)\nprint(\"\u2713 Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training loss\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(losses, linewidth=2, color='blue')\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.title('Training Loss Over Time', fontsize=13, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.axhline(y=-np.log(1/vocab_size), color='r', linestyle='--', alpha=0.5, label='Random baseline')\nplt.legend()\n\n# Smoothed loss\nwindow = 10\nif len(losses) > window:\n    smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')\n    plt.subplot(1, 2, 2)\n    plt.plot(smoothed, linewidth=2, color='green')\n    plt.xlabel('Epoch', fontsize=12)\n    plt.ylabel('Smoothed Loss', fontsize=12)\n    plt.title(f'Smoothed Loss (window={window})', fontsize=13, fontweight='bold')\n    plt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFinal loss: {losses[-1]:.4f}\")\nprint(f\"Random baseline: {-np.log(1/vocab_size):.4f}\")\nprint(f\"Improvement: {100 * (1 - losses[-1] / -np.log(1/vocab_size)):.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Evaluation\n\nLet's test what the model has learned!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on training examples\nprint(\"Testing on training data:\")\nprint(\"=\"*50)\n\ncorrect = 0\nfor i, (input_seq, target_idx) in enumerate(zip(inputs, targets)):\n    # Get input characters\n    input_chars = ''.join([onehot_to_char(input_seq[j], idx_to_char) \n                           for j in range(len(input_seq))])\n    target_char = idx_to_char[target_idx]\n    \n    # Predict\n    probs = model.predict(input_seq)\n    predicted_idx = np.argmax(probs)\n    predicted_char = idx_to_char[predicted_idx]\n    confidence = probs[predicted_idx]\n    \n    # Check if correct\n    is_correct = (predicted_idx == target_idx)\n    if is_correct:\n        correct += 1\n    \n    status = \"\u2713\" if is_correct else \"\u2717\"\n    print(f\"{status} Input: '{input_chars}' \u2192 Predicted: '{predicted_char}' (confidence: {confidence:.2f}) | Target: '{target_char}'\")\n\naccuracy = 100 * correct / len(inputs)\nprint(\"=\"*50)\nprint(f\"Accuracy: {correct}/{len(inputs)} = {accuracy:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Generation\n\nLet's generate text by sampling from the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(model, seed_text, char_to_idx, idx_to_char, vocab_size, \n                  length=20, temperature=1.0):\n    \"\"\"\n    Generate text by sampling from the model.\n    \n    Args:\n        model: Trained CharRNN\n        seed_text: Initial text to start generation\n        char_to_idx: Character to index mapping\n        idx_to_char: Index to character mapping\n        vocab_size: Vocabulary size\n        length: Number of characters to generate\n        temperature: Sampling temperature (higher = more random)\n    \n    Returns:\n        Generated text\n    \"\"\"\n    text = seed_text\n    \n    for _ in range(length):\n        # Take last seq_length characters as input\n        input_text = text[-seq_length:]\n        \n        # Convert to one-hot\n        input_onehot = np.array([char_to_onehot(ch, char_to_idx, vocab_size) \n                                 for ch in input_text])\n        \n        # Predict next character\n        probs = model.predict(input_onehot)\n        \n        # Apply temperature\n        probs = np.power(probs, 1/temperature)\n        probs = probs / probs.sum()\n        \n        # Sample\n        next_idx = np.random.choice(len(probs), p=probs)\n        next_char = idx_to_char[next_idx]\n        \n        text += next_char\n    \n    return text\n\n# Generate text\nprint(\"Text Generation Examples:\")\nprint(\"=\"*50)\n\nfor temp in [0.5, 1.0, 1.5]:\n    generated = generate_text(model, \"hel\", char_to_idx, idx_to_char, \n                              vocab_size, length=20, temperature=temp)\n    print(f\"Temperature {temp}: '{generated}'\")\n\nprint(\"=\"*50)\nprint(\"\\n(Temperature controls randomness: lower = more conservative)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 5 Summary\n\n### What We Built\n\nCongratulations! You just implemented a **complete language model from scratch**, including:\n\n1. \u2705 **Data preparation**: Vocabulary, one-hot encoding, training pairs\n2. \u2705 **Architecture**: CharRNN with input, hidden, and output layers\n3. \u2705 **Forward pass**: Processing sequences through the RNN\n4. \u2705 **Loss function**: Cross-entropy for measuring prediction quality\n5. \u2705 **Backpropagation Through Time**: Full BPTT implementation with gradient clipping\n6. \u2705 **Training**: Gradient descent optimizer\n7. \u2705 **Evaluation**: Testing and text generation\n\nThis is the **same core algorithm** used in:\n- Early language models (2010-2015)\n- Speech recognition systems\n- Machine translation (before transformers)\n- Text generation systems\n\n### Key Insights\n\n#### 1. What the Model Learned\n\nEven on tiny data (\"hello world\"), the RNN learned:\n- Character patterns (\"hel\" \u2192 \"l\")\n- Simple sequences (\"worl\" \u2192 \"d\")\n- Some structure of the text\n\nWith more data, these models can learn:\n- Grammar and syntax\n- Common phrases\n- Longer-range dependencies (within limits)\n\n#### 2. BPTT Challenges\n\n**Vanishing gradients in action**:\n- Even with our simple 3-character sequences, gradients decay\n- For longer sequences (50+ chars), gradients become extremely small\n- This is why we needed gradient clipping\n\n**Computational cost**:\n- Each training example requires:\n  - Forward pass through all timesteps\n  - Backward pass through all timesteps\n  - Gradient accumulation and update\n- For long sequences, this is SLOW\n\n#### 3. Limitations We Encountered\n\n**Limited Context**:\n```\nOur model: seq_length = 3 characters\n  Can only see 3 characters back\n  \"hel\" \u2192 \"l\" \u2713\n  But can't handle: \"The cat, which was...\" (requires longer memory)\n```\n\n**Small Vocabulary**:\n```\nOnly 8 unique characters in \"hello world\"\nReal language: 26 letters + punctuation + numbers = 50-100 characters\nFull Unicode: thousands of characters!\n```\n\n**Tiny Dataset**:\n```\nTraining text: 11 characters\nTraining examples: 8 pairs\nReal LLMs train on billions of tokens!\n```\n\n### From CharRNN to Modern LLMs\n\n**Our CharRNN \u2192 GPT Journey**:\n\n1. **Bigger Data** (2013-2015):\n   - Train on millions of characters/words\n   - Learn more complex patterns\n   - Problem: Still slow (sequential), still limited context\n\n2. **Better Architectures** (2014-2016):\n   - Use LSTMs instead of vanilla RNN\n   - Stack multiple layers (deep networks)\n   - Add dropout, layer normalization\n   - Problem: Still sequential, still limited to ~100-500 tokens\n\n3. **Transformers** (2017):\n   - Replace RNN with self-attention\n   - Parallel processing (fast!)\n   - Direct connections (no vanishing gradients!)\n   - Can handle 1000s of tokens\n   - Result: GPT, BERT, modern LLMs!\n\n### Why This Exercise Matters\n\n**You now understand**:\n- How language models actually work (not magic!)\n- Why training is hard (vanishing gradients, optimization)\n- What backpropagation does (computes gradients)\n- Why we needed transformers (limitations of RNNs)\n\n**When you learn about GPT next**:\n- The attention mechanism will make sense\n- You'll appreciate why it's faster\n- You'll understand the architectural improvements\n- You'll see it as evolution, not revolution\n\n### Extensions (Optional Challenges)\n\nIf you want to go further, try:\n\n1. **Longer sequences**: Increase `seq_length` to 5 or 10\n   - Does it learn better?\n   - Does training slow down?\n\n2. **More data**: Use a longer training text\n   - Try a paragraph or page\n   - Does text generation improve?\n\n3. **LSTM version**: Modify to use `LSTMCell` instead of vanilla RNN\n   - Does it handle longer sequences better?\n   - Is it more stable?\n\n4. **Word-level model**: Instead of characters, predict words\n   - Tokenize by spaces\n   - Does it generate more coherent text?\n\n**You've built something incredible!** \ud83c\udf89"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n\n# Overall Summary & Conclusion\n\n## What We Accomplished\n\nThrough these five exercises, you've gained **deep, hands-on understanding** of the pre-transformer era of deep learning:\n\n### Exercise 1: RNN Experimentation \u2713\n**Key Learning**: Architecture matters\n- Hidden size controls capacity\n- tanh is preferred for RNNs (ReLU causes problems)\n- Longer sequences \u2192 saturation and information loss\n\n### Exercise 2: Vanishing Gradients \u2713\n**Key Learning**: The fundamental limitation\n- Gradients decay exponentially with sequence length\n- \"Goldilocks zone\" of weights is extremely narrow\n- This is a mathematical inevitability, not a bug\n- Motivated the invention of LSTMs and eventually transformers\n\n### Exercise 3: LSTM Gates \u2713\n**Key Learning**: Adaptive memory control\n- Forget, input, and output gates provide fine-grained control\n- Different tasks need different gate patterns\n- LSTMs partially solve vanishing gradients\n- But still sequential and limited\n\n### Exercise 4: Thinking Ahead \u2713\n**Key Learning**: Conceptual foundations\n- Identified core problems: sequential processing, no direct access, bottlenecks\n- Brainstormed solutions: attention, parallel processing, position encoding\n- Prepared mentally for transformers\n\n### Exercise 5: Character-Level Language Model \u2713\n**Key Learning**: End-to-end implementation\n- Built complete language model with backpropagation\n- Saw training dynamics first-hand\n- Understood practical challenges\n- Connected to modern LLMs\n\n## Key Takeaways\n\n### 1. RNNs Are Foundational But Limited\n\n**What they enabled**:\n- Sequential data processing\n- Variable-length inputs\n- Memory through hidden states\n- Early successes in translation, speech, text generation\n\n**Why they struggled**:\n- Vanishing/exploding gradients\n- Sequential processing (slow)\n- Limited context window\n- Information bottlenecks\n\n### 2. LSTMs Improved But Didn't Solve Everything\n\n**Improvements**:\n- Gates provide adaptive memory\n- Can handle ~50-100 timesteps (vs RNN's ~10-20)\n- More stable training\n\n**Remaining problems**:\n- Still sequential (can't parallelize)\n- Still have practical length limits\n- Still have bottlenecks\n- Complex architecture (hard to understand and tune)\n\n### 3. These Limitations Motivated Transformers\n\n**The breakthrough**: Remove recurrence entirely!\n- **Attention mechanism**: Direct access to all positions\n- **Parallel processing**: Process entire sequence at once\n- **Positional encoding**: Explicit position information\n- **Result**: GPT, BERT, ChatGPT, and modern LLMs\n\n## Your Learning Journey\n\n### Where You Started\n- Understanding basic RNN equations\n- Knowing about vanishing gradients conceptually\n- Hearing about transformers and attention\n\n### Where You Are Now\n- **Deep understanding** of RNN/LSTM mechanics\n- **Hands-on experience** with gradient flow and BPTT\n- **Implementation skills**: Built a language model from scratch\n- **Conceptual foundation**: Ready for transformers\n\n## What's Next?\n\nYou're now perfectly prepared for the next notebooks:\n\n### Next: `02_attention_mechanism.ipynb`\n\nYou'll learn:\n- **How attention works**: Query, Key, Value mechanism\n- **Self-attention**: How words attend to each other\n- **Multi-head attention**: Parallel attention in different subspaces\n- **Implementation**: Build attention from scratch\n\n### Then: `03_transformer_architecture.ipynb`\n\nYou'll learn:\n- **Complete transformer**: Encoder and decoder\n- **Positional encoding**: How position information is added\n- **Layer normalization**: Stabilizing deep networks\n- **Residual connections**: Enabling deep stacks\n\n### Eventually: Building GPT\n\nYou'll:\n- Understand decoder-only transformers\n- Learn about tokenization\n- Train a tiny GPT model\n- Generate text with transformers\n\n## Final Thoughts\n\n### You Understand the \"Why\" Now\n\nWhen someone says \"transformers are better than RNNs,\" you don't just nod - you **understand**:\n- Why: parallel processing, direct connections, no vanishing gradients\n- How: attention mechanism, positional encoding\n- Trade-offs: different strengths and weaknesses\n\n### You Can Build Things\n\nYou've implemented:\n- RNNs and LSTMs from scratch\n- Backpropagation through time\n- A complete language model\n\nThese are **real skills** that transfer to:\n- Reading research papers\n- Understanding model architectures\n- Debugging training issues\n- Building custom models\n\n### You're Part of the Story\n\nThe journey from RNNs \u2192 LSTMs \u2192 Transformers is:\n- Not just history\n- Not just trivia\n- **The foundation of modern AI**\n\nEvery time you use:\n- ChatGPT\n- Google Translate\n- Voice assistants\n- Code completion\n\n...you're using ideas built on what you just learned!\n\n## Congratulations! \ud83c\udf89\n\nYou've completed the pre-transformer era! You have:\n- \u2705 Solid understanding of RNNs and LSTMs\n- \u2705 Hands-on implementation experience\n- \u2705 Deep knowledge of why transformers were needed\n- \u2705 Strong foundation for learning attention\n\n**You're ready for the attention mechanism!**\n\nSee you in the next notebook! \ud83d\ude80"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}